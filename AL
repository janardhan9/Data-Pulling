
#config.py

import os
from datetime import datetime

# Project Configuration
PROJECT_NAME = "Healthcare Bills Dataset Extraction"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Healthcare Keywords
KEYWORDS = [
    'Prior authorization',
    'Utilization review',
    'Utilization management',
    'Medical necessity review',
    'Prompt pay',
    'Prompt payment',
    'Clean claims',
    'Clean claim',
    'Coordination of benefits',
    'Artificial intelligence',
    'Clinical decision support',
    'Automated decision making',
    'Automate decision support'
]

# Target Years
TARGET_YEARS = [2025, 2026]

# Directory Configuration
DATASETS_DIR = "datasets"
DATA_DIR = "data"
LOGS_DIR = "logs"
OUTPUT_FILE = "data/healthcare_bills_output.xlsx"
LOG_FILE = "logs/extraction.log"

# Dataset Processing Settings
MAX_DATASETS_PARALLEL = 4
DATASET_CACHE_SIZE = 1000
PARALLEL_PROCESSING = True

# Status Code Mapping
STATUS_MAPPING = {
    1: "Introduced",
    2: "Engrossed", 
    3: "Enrolled",
    4: "Passed"
}

# State Mapping
STATE_MAPPING = {
    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',
    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',
    'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',
    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',
    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',
    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',
    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',
    'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',
    'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',
    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',
    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',
    'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',
    'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia', 'US': 'United States'
}

# Logging Configuration
LOG_LEVEL = "INFO"



#Datasetmanager

import os
import json
import requests
import logging
from datetime import datetime, timedelta
from pathlib import Path
from config import *

class DatasetManager:
    def __init__(self, data_dir=DATASETS_DIR):
        """Initialize dataset manager"""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.setup_logging()
        
    def setup_logging(self):
        """Setup logging for dataset operations"""
        os.makedirs(LOGS_DIR, exist_ok=True)
        logging.basicConfig(
            level=getattr(logging, LOG_LEVEL),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_FILE),
                logging.StreamHandler()
            ]
        )
        
    def get_available_datasets(self):
        """List all downloaded datasets"""
        datasets = list(self.data_dir.glob("*-legiscan.json"))
        logging.info(f"Found {len(datasets)} datasets in {self.data_dir}")
        return datasets
    
    def check_dataset_exists(self, target_date):
        """Check if dataset for specific date already exists"""
        dataset_filename = f"{target_date}-legiscan.json"
        dataset_path = self.data_dir / dataset_filename
        return dataset_path.exists()
    
    def download_dataset_info(self):
        """Get information about available datasets from LegiScan"""
        try:
            # LegiScan datasets page
            info_url = "https://legiscan.com/datasets"
            print(f"üìä LegiScan datasets information:")
            print(f"   Visit: {info_url}")
            print(f"   Weekly datasets are available for download")
            print(f"   Format: YYYY-MM-DD-legiscan.json")
            print(f"   Updated: Every Sunday")
            return True
        except Exception as e:
            logging.error(f"Failed to get dataset info: {e}")
            return False
    
    def download_weekly_dataset(self, target_date):
        """Download LegiScan weekly dataset for specific date"""
        dataset_filename = f"{target_date}-legiscan.json"
        dataset_path = self.data_dir / dataset_filename
        
        if dataset_path.exists():
            print(f"‚úÖ Dataset {dataset_filename} already exists")
            return dataset_path
            
        print(f"üì• Downloading dataset for {target_date}...")
        print(f"   Note: This is a placeholder - you'll need to manually download")
        print(f"   from https://legiscan.com/datasets and save as {dataset_filename}")
        
        # For now, create a placeholder file structure
        # In real implementation, you would download from LegiScan
        return dataset_path
    
    def validate_dataset(self, dataset_path):
        """Validate downloaded dataset format"""
        try:
            with open(dataset_path, 'r') as f:
                data = json.load(f)
            
            # Basic validation
            if not isinstance(data, dict):
                return False
                
            print(f"‚úÖ Dataset {dataset_path.name} is valid")
            return True
            
        except Exception as e:
            logging.error(f"Dataset validation failed: {e}")
            return False
    
    def get_dataset_summary(self, dataset_path):
        """Get summary information about a dataset"""
        try:
            with open(dataset_path, 'r') as f:
                data = json.load(f)
            
            # Extract summary info
            summary = {
                'file': dataset_path.name,
                'size': dataset_path.stat().st_size,
                'date': dataset_path.stat().st_mtime
            }
            
            return summary
            
        except Exception as e:
            logging.error(f"Failed to get dataset summary: {e}")
            return None

# Test the dataset manager
if __name__ == "__main__":
    print("=" * 60)
    print("HEALTHCARE BILLS DATASET MANAGER - TESTING")
    print("=" * 60)
    
    # Initialize manager
    manager = DatasetManager()
    
    # Check current datasets
    datasets = manager.get_available_datasets()
    print(f"Current datasets: {len(datasets)}")
    
    # Show dataset info
    manager.download_dataset_info()
    
    print("\nDataset manager setup complete!")
    print("Ready for Step 2: Dataset Download and Processing")



#bill_processor

import pandas as pd
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import *

class BillProcessor:
    def __init__(self):
        self.processed_bills = []
        self.processing_stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'duplicates_removed': 0
        }
        
    def extract_sponsors(self, sponsors_data):
        """Extract sponsors using LegiScan data structure"""
        if not sponsors_data:
            return "No sponsors listed"
        
        # Sort by sponsor_order to get primary sponsor first
        sorted_sponsors = sorted(sponsors_data, key=lambda x: x.get('sponsor_order', 999))
        
        # Get first sponsor with a name (primary sponsor)
        for sponsor in sorted_sponsors:
            if sponsor.get('name'):
                return sponsor['name']
        
        return "No sponsors listed"
    
    def extract_last_action(self, history_data):
        """Extract the most recent action from history (without date)"""
        if not history_data:
            return "No action recorded"
        
        try:
            # If it's a list of dictionaries
            if isinstance(history_data, list) and history_data:
                # Sort by date and get the most recent
                sorted_actions = sorted(history_data, 
                                    key=lambda x: x.get('date', '1900-01-01'), 
                                    reverse=True)
                latest_action = sorted_actions[0]
                action_text = latest_action.get('action', 'No action text')
                
                # Truncate long actions for readability
                if len(action_text) > 100:
                    action_text = action_text[:100] + "..."
                
                return action_text
            
            # If it's already a string
            if isinstance(history_data, str):
                return history_data[:100] + "..." if len(history_data) > 100 else history_data
                
            # If it's a dictionary
            if isinstance(history_data, dict):
                action = history_data.get('action', 'No action recorded')
                return action[:100] + "..." if len(action) > 100 else action
                
        except Exception as e:
            logging.warning(f"Failed to extract last action: {e}")
            
        return "No action recorded"
    
    def process_bill_data(self, bill_data):
        """Process individual bill data into required format"""
        try:
            # Handle different data structures from datasets
            if 'bill' in bill_data:
                bill = bill_data['bill']
            else:
                bill = bill_data
                
            session = bill.get('session', {})
            
            # Get state abbreviation and convert to full name
            state_abbr = bill.get('state', '')
            state_full_name = STATE_MAPPING.get(state_abbr, state_abbr)
            
            # Extract required fields
            processed_bill = {
                'Year': session.get('year_start', ''),
                'State': state_full_name,
                'Bill Number': bill.get('bill_number', ''),
                'Bill Title/Topic': bill.get('title', ''),
                'Summary': bill.get('description', ''),
                'Sponsors': self.extract_sponsors(bill.get('sponsors', [])),
                'Last Action': self.extract_last_action(bill.get('history', [])),
                'Bill Link': bill.get('url', ''),
                'Current Status': STATUS_MAPPING.get(bill.get('status'), 'Unknown'),
                'Extracted Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            self.processing_stats['successful'] += 1
            return processed_bill
            
        except Exception as e:
            logging.error(f"Failed to process bill data: {e}")
            self.processing_stats['failed'] += 1
            return None
    
    def add_bill(self, processed_bill):
        """Add a processed bill to the collection"""
        if processed_bill:
            self.processed_bills.append(processed_bill)
            self.processing_stats['total_processed'] += 1
            
    def remove_duplicates(self):
        """Remove duplicate bills based on state and bill number"""
        seen = set()
        unique_bills = []
        
        for bill in self.processed_bills:
            identifier = (bill['State'], bill['Bill Number'])
            if identifier not in seen:
                seen.add(identifier)
                unique_bills.append(bill)
            else:
                self.processing_stats['duplicates_removed'] += 1
                # Silent duplicate removal - no logging
        
        self.processed_bills = unique_bills
        
    def save_to_excel(self, output_file=OUTPUT_FILE):
        """Save processed bills to Excel file"""
        if not self.processed_bills:
            logging.warning("No bills to save")
            print("No bills found matching your criteria.")
            return
        
        # Remove duplicates before saving
        self.remove_duplicates()
        
        try:
            df = pd.DataFrame(self.processed_bills)
            
            # Ensure output directory exists
            import os
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # Save to Excel with formatting
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Bills', index=False)
                
                # Get the workbook and worksheet
                worksheet = writer.sheets['Bills']
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logging.info(f"Saved {len(self.processed_bills)} bills to {output_file}")
            print(f"Successfully saved {len(self.processed_bills)} bills to {output_file}")
            
        except Exception as e:
            logging.error(f"Failed to save Excel file: {e}")
            print(f"Error saving Excel file: {e}")
    
    def get_summary(self):
        """Get a summary of processed bills"""
        if not self.processed_bills:
            return "No bills processed"
        
        total_bills = len(self.processed_bills)
        states = set(bill['State'] for bill in self.processed_bills)
        years = set(bill['Year'] for bill in self.processed_bills)
        
        return f"Total Bills: {total_bills} | States: {len(states)} | Years: {sorted(years)}"
    
    def get_processing_stats(self):
        """Get detailed processing statistics"""
        return self.processing_stats


#dataset_Loader

import json
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import *

class DatasetLoader:
    def __init__(self, datasets_dir=DATASETS_DIR):
        self.datasets_dir = Path(datasets_dir)
        self.all_bills = []
        
    def load_single_dataset(self, dataset_path):
        """Load a single dataset file"""
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            bills = self.extract_bills_from_dataset(data)
            logging.info(f"Loaded {len(bills)} bills from {dataset_path.name}")
            return bills
            
        except Exception as e:
            logging.error(f"Failed to load dataset {dataset_path}: {e}")
            return []
    
    def extract_bills_from_dataset(self, dataset):
        """Extract bills from LegiScan dataset structure"""
        bills = []
        
        try:
            # LegiScan dataset structure varies, so we need to handle different formats
            if isinstance(dataset, dict):
                # Look for bills in various possible structures
                if 'bills' in dataset:
                    bills.extend(dataset['bills'])
                elif 'searchresult' in dataset:
                    search_result = dataset['searchresult']
                    if 'results' in search_result:
                        bills.extend(search_result['results'])
                elif 'results' in dataset:
                    bills.extend(dataset['results'])
                else:
                    # Try to find bills in nested structures
                    for key, value in dataset.items():
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'bill_id' in item:
                                    bills.append(item)
                                    
        except Exception as e:
            logging.error(f"Error extracting bills from dataset: {e}")
            
        return bills
    
    def load_all_datasets(self):
        """Load all available datasets"""
        dataset_files = list(self.datasets_dir.glob("*-legiscan.json"))
        
        if not dataset_files:
            print("‚ö†Ô∏è  No datasets found in datasets/ folder")
            print("   Please download datasets from https://legiscan.com/datasets")
            return []
        
        print(f"üìÇ Loading {len(dataset_files)} datasets...")
        
        all_bills = []
        
        # Load datasets in parallel for better performance
        with ThreadPoolExecutor(max_workers=MAX_DATASETS_PARALLEL) as executor:
            future_to_dataset = {
                executor.submit(self.load_single_dataset, dataset_path): dataset_path
                for dataset_path in dataset_files
            }
            
            for future in as_completed(future_to_dataset):
                dataset_path = future_to_dataset[future]
                try:
                    bills = future.result()
                    all_bills.extend(bills)
                except Exception as e:
                    logging.error(f"Error processing dataset {dataset_path}: {e}")
        
        # Remove duplicates by bill_id
        unique_bills = {}
        for bill in all_bills:
            bill_id = bill.get('bill_id')
            if bill_id and bill_id not in unique_bills:
                unique_bills[bill_id] = bill
        
        final_bills = list(unique_bills.values())
        print(f"üìä Total unique bills loaded: {len(final_bills)}")
        
        return final_bills
    
    def filter_bills_by_keyword(self, bills, keyword):
        """Filter bills by healthcare keyword"""
        matching_bills = []
        
        for bill in bills:
            if self.bill_contains_keyword(bill, keyword):
                matching_bills.append(bill)
        
        return matching_bills
    
    def bill_contains_keyword(self, bill, keyword):
        """Check if bill contains the healthcare keyword"""
        # Search in multiple fields
        search_fields = [
            bill.get('title', ''),
            bill.get('description', ''),
            bill.get('text', ''),
            bill.get('summary', '')
        ]
        
        # Also search in sponsor information
        sponsors = bill.get('sponsors', [])
        for sponsor in sponsors:
            if isinstance(sponsor, dict):
                search_fields.append(sponsor.get('name', ''))
        
        # Combine all text and search (case-insensitive)
        search_text = ' '.join(search_fields).lower()
        return keyword.lower() in search_text
    
    def filter_bills_by_year(self, bills, target_years):
        """Filter bills by target years"""
        filtered_bills = []
        
        for bill in bills:
            session = bill.get('session', {})
            bill_year = session.get('year_start')
            
            if bill_year in target_years:
                filtered_bills.append(bill)
        
        return filtered_bills
    
    def search_bills_comprehensive(self, keyword):
        """Comprehensive search for healthcare bills"""
        # Load all datasets
        all_bills = self.load_all_datasets()
        
        if not all_bills:
            return []
        
        # Filter by keyword
        keyword_bills = self.filter_bills_by_keyword(all_bills, keyword)
        
        # Filter by target years
        final_bills = self.filter_bills_by_year(keyword_bills, TARGET_YEARS)
        
        logging.info(f"Found {len(final_bills)} bills for keyword '{keyword}'")
        return final_bills

# Test the dataset loader
if __name__ == "__main__":
    print("=" * 60)
    print("DATASET LOADER - TESTING")
    print("=" * 60)
    
    # Initialize loader
    loader = DatasetLoader()
    
    # Test loading (will show warning if no datasets)
    bills = loader.load_all_datasets()
    
    if bills:
        print(f"‚úÖ Successfully loaded {len(bills)} bills")
        
        # Test keyword search
        test_keyword = "Prior authorization"
        matching_bills = loader.filter_bills_by_keyword(bills, test_keyword)
        print(f"‚úÖ Found {len(matching_bills)} bills matching '{test_keyword}'")
    else:
        print("‚ÑπÔ∏è  No datasets available for testing")
        print("   This is normal - datasets will be added in the next step")
    
    print("\nDataset loader ready!")
