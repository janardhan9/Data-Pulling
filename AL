import urllib3
# Disable SSL warnings (insecure requests)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

import requests
import pandas as pd
import datetime
import re
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ----- Configuration -----
BASE_URL    = "https://www.akleg.gov"   # Alaska Legislature base URL
LEG_NUMBER  = 34                         # 34th Legislature (2025-2026)
MAX_WORKERS = 10                         # threads for detail-page fetch

# Build session: disable SSL verify & retry on server errors
session = requests.Session()
session.verify = False
retries = Retry(total=5, backoff_factor=1, status_forcelist=[502,503,504], allowed_methods=["GET"])
adapter = HTTPAdapter(max_retries=retries)
session.mount("http://", adapter)
session.mount("https://", adapter)

# Timestamped filename to avoid overwrite
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_CSV = f"alaska_bills_34th_legislature_{ts}.csv"

# ----- Step 1: Fetch and parse list page -----
def fetch_list_page() -> str:
    """Download the HTML for the 34th Legislature bill list page."""
    url = f"{BASE_URL}/basis/Bill/Range/{LEG_NUMBER}?session=&bill1=&bill2="
    resp = session.get(url, timeout=10)
    resp.raise_for_status()
    return resp.text


def parse_list(html: str) -> list:
    """Parse the bill list table and build entries with correct, encoded URLs."""
    soup = BeautifulSoup(html, 'html.parser')
    table = soup.find('table')  # first table is the bill list
    if not table:
        return []

    entries = []
    # Skip header row
    for tr in table.find_all('tr')[1:]:
        cols = tr.find_all('td')
        if len(cols) < 5:
            continue
        # Extract the <a> tag
        a = cols[0].find('a')
        raw_href = a['href'] if a else ''
        # Build absolute URL and encode spaces to %20
        detail_url = BASE_URL + raw_href.strip()
        detail_url = re.sub(r"\s+", '%20', detail_url)

        entries.append({
            'Bill':           a.get_text(strip=True) if a else '',
            'DetailURL':      detail_url,
            'Bill_link':      detail_url,
            'Short Title':    cols[1].get_text(strip=True),
            'Sponsor(s)':     cols[2].get_text(strip=True),
            'Current Status': cols[4].get_text(strip=True),
            'Status Date':    cols[5].get_text(strip=True),
        })
    # Debug sample
    print("Sample encoded links:", [e['DetailURL'] for e in entries[:5]])
    return entries

# ----- Step 2: Fetch and parse detail page -----
def parse_detail(entry: dict) -> dict:
    """Fetch each bill's detail page and extract Year & full Title."""
    # 1) Year from Status Date (last 4 digits)
    status = entry.get('Status Date', '')
    m = re.search(r"(\d{4})$", status)
    entry['Year'] = m.group(1) if m else ''

    # 2) Download and parse detail page HTML
    resp = session.get(entry['DetailURL'], timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')

    # 3) Extract Title from <li><span>Title</span><strong>…</strong></li>
    entry['Title'] = ''
    for li in soup.find_all('li'):
        span = li.find('span')
        if span and span.get_text(strip=True).upper() == 'TITLE':
            strong = li.find('strong')
            if strong:
                entry['Title'] = strong.get_text(strip=True)
            break

    # 4) Extraction timestamp
    entry['extracted_date'] = datetime.datetime.today().date().isoformat()
    return entry

# ----- Orchestration -----
def scrape_34th() -> pd.DataFrame:
    """End-to-end scraping: list fetch, detail fetch in parallel, assemble DataFrame."""
    html    = fetch_list_page()
    entries = parse_list(html)
    print(f"Found {len(entries)} bills on list page.")

    records = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(parse_detail, e.copy()) for e in entries]
        for future in as_completed(futures):
            try:
                records.append(future.result())
            except Exception as err:
                print("Detail fetch error:", err)

    # Build DataFrame with defined column order
    df = pd.DataFrame(records, columns=[
        'Bill', 'Current Status', 'Status Date', 'Sponsor(s)',
        'Title', 'Short Title', 'Year', 'Bill_link', 'extracted_date'
    ])
    return df

# ----- Main -----
if __name__ == '__main__':
    df = scrape_34th()
    print(df.head())
    df.to_csv(OUTPUT_CSV, index=False)
    print(f"✅ Saved {len(df)} bills to {OUTPUT_CSV}")
