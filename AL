import os
import datetime
import requests
import pandas as pd
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configuration for 34th Legislature only
BASE_URL    = "https://www.akleg.gov"
LEG_NUMBER  = 34                # only the 34th Legislature
MAX_WORKERS = 10                # threads for detail-page fetch

# Build a requests.Session with retry-on-502/503/504
session = requests.Session()
retries = Retry(
    total=5,                    # up to 5 retries
    backoff_factor=1,           # 1s, 2s, 4s, 8s, ...
    status_forcelist=[502,503,504],
    allowed_methods=["GET"]
)
adapter = HTTPAdapter(max_retries=retries)
session.mount("https://", adapter)
session.mount("http://", adapter)

# Timestamped filename so each run is unique
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_CSV = f'alaska_bills_34th_legislature_{ts}.csv'

def fetch_list_page() -> str:
    """Download the list-page HTML for the 34th Legislature, with retries."""
    url = f"{BASE_URL}/basis/Bill/Range/{LEG_NUMBER}?session=&bill1=&bill2="
    resp = session.get(url, timeout=10)
    resp.raise_for_status()
    return resp.text

def parse_list(html: str) -> list[dict]:
    """Parse the list-page table into bill metadata entries."""
    soup = BeautifulSoup(html, 'html.parser')
    table = next(
        (t for t in soup.find_all('table')
         if 'Short Title' in t.find('tr').get_text()), 
        None
    )
    if not table:
        return []
    rows = table.find_all('tr')[1:]
    entries = []
    for tr in rows:
        cols = tr.find_all('td')
        if len(cols) < 6:
            continue
        a = cols[0].find('a')
        href = a['href']
        entries.append({
            'Bill':           a.get_text(strip=True),
            'DetailURL':      BASE_URL + href,
            'Bill_link':      BASE_URL + href,
            'Short Title':    cols[1].get_text(strip=True),
            'Sponsor(s)':     cols[2].get_text(strip=True),
            'Current Status': cols[4].get_text(strip=True),
            'Status Date':    cols[5].get_text(strip=True)
        })
    return entries

def parse_detail(entry: dict) -> dict:
    """Fetch detail page, extract Year and full Title, then update entry."""
    url = entry['DetailURL']
    resp = session.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')

    # Year is fixed at 2025 for the 34th Legislature
    entry['Year'] = 2025

    # Extract the full Title from the <li><span>Title</span><strong>…</strong></li>
    title = ''
    for li in soup.find_all('li'):
        span = li.find('span')
        if span and span.get_text(strip=True).upper() == 'TITLE':
            strong = li.find('strong')
            if strong:
                title = strong.get_text(strip=True)
            break
    entry['Title'] = title
    entry['extracted_date'] = datetime.date.today().isoformat()
    return entry

def scrape_34th() -> pd.DataFrame:
    """Scrape all bills for the 34th Legislature into a DataFrame."""
    html = fetch_list_page()
    entries = parse_list(html)

    # Fetch details concurrently
    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(parse_detail, e.copy()) for e in entries]
        for future in as_completed(futures):
            try:
                results.append(future.result())
            except Exception:
                # Skip any that still fail after retries
                pass

    # Build DataFrame with explicit column order
    df = pd.DataFrame(results, columns=[
        'Bill', 'Current Status', 'Status Date', 'Sponsor(s)',
        'Title', 'Short Title', 'Year', 'Bill_link', 'extracted_date'
    ])
    return df

if __name__ == '__main__':
    df = scrape_34th()               # Run the scrape
    df.to_csv(OUTPUT_CSV, index=False)  # Write timestamped CSV
    print(f"✅ Saved {len(df)} bills to {OUTPUT_CSV}")


