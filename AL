#sample 

import json
import os
from datetime import datetime
from config import *

def create_sample_dataset():
    """Create a sample dataset for testing the system"""
    
    # Sample healthcare bills data
    sample_bills = [
        {
            "bill_id": "2025001",
            "title": "Prior authorization reform for healthcare services",
            "description": "A bill to reform prior authorization requirements for medical services",
            "state": "CA",
            "bill_number": "AB1234",
            "session": {"year_start": 2025},
            "status": 1,
            "sponsors": [
                {
                    "name": "John Smith",
                    "role": "Rep", 
                    "sponsor_order": 1
                }
            ],
            "history": [
                {
                    "date": "2025-01-15",
                    "action": "Introduced in Assembly"
                }
            ],
            "url": "https://legiscan.com/CA/bill/AB1234/2025"
        },
        {
            "bill_id": "2025002", 
            "title": "Artificial intelligence in healthcare regulation",
            "description": "Establishing guidelines for AI use in medical diagnosis",
            "state": "NY",
            "bill_number": "S567",
            "session": {"year_start": 2025},
            "status": 2,
            "sponsors": [
                {
                    "name": "Mary Johnson",
                    "role": "Rep",
                    "sponsor_order": 1
                }
            ],
            "history": [
                {
                    "date": "2025-01-20",
                    "action": "Passed Senate Committee"
                }
            ],
            "url": "https://legiscan.com/NY/bill/S567/2025"
        },
        {
            "bill_id": "2025003",
            "title": "Clean claims processing requirements",
            "description": "Standards for electronic health insurance claim processing",
            "state": "TX", 
            "bill_number": "HB890",
            "session": {"year_start": 2025},
            "status": 1,
            "sponsors": [
                {
                    "name": "Robert Davis",
                    "role": "Rep",
                    "sponsor_order": 1
                }
            ],
            "history": [
                {
                    "date": "2025-01-25",
                    "action": "Filed with Secretary of State"
                }
            ],
            "url": "https://legiscan.com/TX/bill/HB890/2025"
        }
    ]
    
    # Create sample dataset structure
    sample_dataset = {
        "date": "2025-01-28",
        "total_bills": len(sample_bills),
        "bills": sample_bills
    }
    
    # Save sample dataset
    os.makedirs(DATASETS_DIR, exist_ok=True)
    sample_file = os.path.join(DATASETS_DIR, "2025-01-28-legiscan.json")
    
    with open(sample_file, 'w') as f:
        json.dump(sample_dataset, f, indent=2)
    
    print(f"‚úÖ Created sample dataset: {sample_file}")
    print(f"üìä Contains {len(sample_bills)} sample healthcare bills")
    print("üß™ You can now test the system with: python main.py")

if __name__ == "__main__":
    print("Creating sample dataset for testing...")
    create_sample_dataset()




#config.py

import os
from datetime import datetime

# Project Configuration
PROJECT_NAME = "Healthcare Bills Dataset Extraction"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Healthcare Keywords
KEYWORDS = [
    'Prior authorization',
    'Utilization review',
    'Utilization management',
    'Medical necessity review',
    'Prompt pay',
    'Prompt payment',
    'Clean claims',
    'Clean claim',
    'Coordination of benefits',
    'Artificial intelligence',
    'Clinical decision support',
    'Automated decision making',
    'Automate decision support'
]

# Target Years
TARGET_YEARS = [2025, 2026]

# Directory Configuration
DATASETS_DIR = "datasets"
DATA_DIR = "data"
LOGS_DIR = "logs"
OUTPUT_FILE = "data/healthcare_bills_output.xlsx"
LOG_FILE = "logs/extraction.log"

# Dataset Processing Settings
MAX_DATASETS_PARALLEL = 4
DATASET_CACHE_SIZE = 1000
PARALLEL_PROCESSING = True

# Status Code Mapping
STATUS_MAPPING = {
    1: "Introduced",
    2: "Engrossed", 
    3: "Enrolled",
    4: "Passed"
}

# State Mapping
STATE_MAPPING = {
    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',
    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',
    'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',
    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',
    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',
    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',
    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',
    'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',
    'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',
    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',
    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',
    'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',
    'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia', 'US': 'United States'
}

# Logging Configuration
LOG_LEVEL = "INFO"



#Datasetmanager

import os
import json
import requests
import logging
from datetime import datetime, timedelta
from pathlib import Path
from config import *

class DatasetManager:
    def __init__(self, data_dir=DATASETS_DIR):
        """Initialize dataset manager"""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.setup_logging()
        
    def setup_logging(self):
        """Setup logging for dataset operations"""
        os.makedirs(LOGS_DIR, exist_ok=True)
        logging.basicConfig(
            level=getattr(logging, LOG_LEVEL),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_FILE),
                logging.StreamHandler()
            ]
        )
        
    def get_available_datasets(self):
        """List all downloaded datasets"""
        datasets = list(self.data_dir.glob("*-legiscan.json"))
        logging.info(f"Found {len(datasets)} datasets in {self.data_dir}")
        return datasets
    
    def check_dataset_exists(self, target_date):
        """Check if dataset for specific date already exists"""
        dataset_filename = f"{target_date}-legiscan.json"
        dataset_path = self.data_dir / dataset_filename
        return dataset_path.exists()
    
    def download_dataset_info(self):
        """Get information about available datasets from LegiScan"""
        try:
            # LegiScan datasets page
            info_url = "https://legiscan.com/datasets"
            print(f"üìä LegiScan datasets information:")
            print(f"   Visit: {info_url}")
            print(f"   Weekly datasets are available for download")
            print(f"   Format: YYYY-MM-DD-legiscan.json")
            print(f"   Updated: Every Sunday")
            return True
        except Exception as e:
            logging.error(f"Failed to get dataset info: {e}")
            return False
    
    def download_weekly_dataset(self, target_date):
        """Download LegiScan weekly dataset for specific date"""
        dataset_filename = f"{target_date}-legiscan.json"
        dataset_path = self.data_dir / dataset_filename
        
        if dataset_path.exists():
            print(f"‚úÖ Dataset {dataset_filename} already exists")
            return dataset_path
            
        print(f"üì• Downloading dataset for {target_date}...")
        print(f"   Note: This is a placeholder - you'll need to manually download")
        print(f"   from https://legiscan.com/datasets and save as {dataset_filename}")
        
        # For now, create a placeholder file structure
        # In real implementation, you would download from LegiScan
        return dataset_path
    
    def validate_dataset(self, dataset_path):
        """Validate downloaded dataset format"""
        try:
            with open(dataset_path, 'r') as f:
                data = json.load(f)
            
            # Basic validation
            if not isinstance(data, dict):
                return False
                
            print(f"‚úÖ Dataset {dataset_path.name} is valid")
            return True
            
        except Exception as e:
            logging.error(f"Dataset validation failed: {e}")
            return False
    
    def get_dataset_summary(self, dataset_path):
        """Get summary information about a dataset"""
        try:
            with open(dataset_path, 'r') as f:
                data = json.load(f)
            
            # Extract summary info
            summary = {
                'file': dataset_path.name,
                'size': dataset_path.stat().st_size,
                'date': dataset_path.stat().st_mtime
            }
            
            return summary
            
        except Exception as e:
            logging.error(f"Failed to get dataset summary: {e}")
            return None

# Test the dataset manager
if __name__ == "__main__":
    print("=" * 60)
    print("HEALTHCARE BILLS DATASET MANAGER - TESTING")
    print("=" * 60)
    
    # Initialize manager
    manager = DatasetManager()
    
    # Check current datasets
    datasets = manager.get_available_datasets()
    print(f"Current datasets: {len(datasets)}")
    
    # Show dataset info
    manager.download_dataset_info()
    
    print("\nDataset manager setup complete!")
    print("Ready for Step 2: Dataset Download and Processing")



#bill_processor

import pandas as pd
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import *

class BillProcessor:
    def __init__(self):
        self.processed_bills = []
        self.processing_stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'duplicates_removed': 0
        }
        
    def extract_sponsors(self, sponsors_data):
        """Extract sponsors using LegiScan data structure"""
        if not sponsors_data:
            return "No sponsors listed"
        
        # Sort by sponsor_order to get primary sponsor first
        sorted_sponsors = sorted(sponsors_data, key=lambda x: x.get('sponsor_order', 999))
        
        # Get first sponsor with a name (primary sponsor)
        for sponsor in sorted_sponsors:
            if sponsor.get('name'):
                return sponsor['name']
        
        return "No sponsors listed"
    
    def extract_last_action(self, history_data):
        """Extract the most recent action from history (without date)"""
        if not history_data:
            return "No action recorded"
        
        try:
            # If it's a list of dictionaries
            if isinstance(history_data, list) and history_data:
                # Sort by date and get the most recent
                sorted_actions = sorted(history_data, 
                                    key=lambda x: x.get('date', '1900-01-01'), 
                                    reverse=True)
                latest_action = sorted_actions[0]
                action_text = latest_action.get('action', 'No action text')
                
                # Truncate long actions for readability
                if len(action_text) > 100:
                    action_text = action_text[:100] + "..."
                
                return action_text
            
            # If it's already a string
            if isinstance(history_data, str):
                return history_data[:100] + "..." if len(history_data) > 100 else history_data
                
            # If it's a dictionary
            if isinstance(history_data, dict):
                action = history_data.get('action', 'No action recorded')
                return action[:100] + "..." if len(action) > 100 else action
                
        except Exception as e:
            logging.warning(f"Failed to extract last action: {e}")
            
        return "No action recorded"
    
    def process_bill_data(self, bill_data):
        """Process individual bill data into required format"""
        try:
            # Handle different data structures from datasets
            if 'bill' in bill_data:
                bill = bill_data['bill']
            else:
                bill = bill_data
                
            session = bill.get('session', {})
            
            # Get state abbreviation and convert to full name
            state_abbr = bill.get('state', '')
            state_full_name = STATE_MAPPING.get(state_abbr, state_abbr)
            
            # Extract required fields
            processed_bill = {
                'Year': session.get('year_start', ''),
                'State': state_full_name,
                'Bill Number': bill.get('bill_number', ''),
                'Bill Title/Topic': bill.get('title', ''),
                'Summary': bill.get('description', ''),
                'Sponsors': self.extract_sponsors(bill.get('sponsors', [])),
                'Last Action': self.extract_last_action(bill.get('history', [])),
                'Bill Link': bill.get('url', ''),
                'Current Status': STATUS_MAPPING.get(bill.get('status'), 'Unknown'),
                'Extracted Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            self.processing_stats['successful'] += 1
            return processed_bill
            
        except Exception as e:
            logging.error(f"Failed to process bill data: {e}")
            self.processing_stats['failed'] += 1
            return None
    
    def add_bill(self, processed_bill):
        """Add a processed bill to the collection"""
        if processed_bill:
            self.processed_bills.append(processed_bill)
            self.processing_stats['total_processed'] += 1
            
    def remove_duplicates(self):
        """Remove duplicate bills based on state and bill number"""
        seen = set()
        unique_bills = []
        
        for bill in self.processed_bills:
            identifier = (bill['State'], bill['Bill Number'])
            if identifier not in seen:
                seen.add(identifier)
                unique_bills.append(bill)
            else:
                self.processing_stats['duplicates_removed'] += 1
                # Silent duplicate removal - no logging
        
        self.processed_bills = unique_bills
        
    def save_to_excel(self, output_file=OUTPUT_FILE):
        """Save processed bills to Excel file"""
        if not self.processed_bills:
            logging.warning("No bills to save")
            print("No bills found matching your criteria.")
            return
        
        # Remove duplicates before saving
        self.remove_duplicates()
        
        try:
            df = pd.DataFrame(self.processed_bills)
            
            # Ensure output directory exists
            import os
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # Save to Excel with formatting
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Bills', index=False)
                
                # Get the workbook and worksheet
                worksheet = writer.sheets['Bills']
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logging.info(f"Saved {len(self.processed_bills)} bills to {output_file}")
            print(f"Successfully saved {len(self.processed_bills)} bills to {output_file}")
            
        except Exception as e:
            logging.error(f"Failed to save Excel file: {e}")
            print(f"Error saving Excel file: {e}")
    
    def get_summary(self):
        """Get a summary of processed bills"""
        if not self.processed_bills:
            return "No bills processed"
        
        total_bills = len(self.processed_bills)
        states = set(bill['State'] for bill in self.processed_bills)
        years = set(bill['Year'] for bill in self.processed_bills)
        
        return f"Total Bills: {total_bills} | States: {len(states)} | Years: {sorted(years)}"
    
    def get_processing_stats(self):
        """Get detailed processing statistics"""
        return self.processing_stats


#dataset_Loader

import json
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import *

class DatasetLoader:
    def __init__(self, datasets_dir=DATASETS_DIR):
        self.datasets_dir = Path(datasets_dir)
        self.all_bills = []
        
    def load_single_dataset(self, dataset_path):
        """Load a single dataset file"""
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            bills = self.extract_bills_from_dataset(data)
            logging.info(f"Loaded {len(bills)} bills from {dataset_path.name}")
            return bills
            
        except Exception as e:
            logging.error(f"Failed to load dataset {dataset_path}: {e}")
            return []
    
    def extract_bills_from_dataset(self, dataset):
        """Extract bills from LegiScan dataset structure"""
        bills = []
        
        try:
            # LegiScan dataset structure varies, so we need to handle different formats
            if isinstance(dataset, dict):
                # Look for bills in various possible structures
                if 'bills' in dataset:
                    bills.extend(dataset['bills'])
                elif 'searchresult' in dataset:
                    search_result = dataset['searchresult']
                    if 'results' in search_result:
                        bills.extend(search_result['results'])
                elif 'results' in dataset:
                    bills.extend(dataset['results'])
                else:
                    # Try to find bills in nested structures
                    for key, value in dataset.items():
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'bill_id' in item:
                                    bills.append(item)
                                    
        except Exception as e:
            logging.error(f"Error extracting bills from dataset: {e}")
            
        return bills
    
    def load_all_datasets(self):
        """Load all available datasets"""
        dataset_files = list(self.datasets_dir.glob("*-legiscan.json"))
        
        if not dataset_files:
            print("‚ö†Ô∏è  No datasets found in datasets/ folder")
            print("   Please download datasets from https://legiscan.com/datasets")
            return []
        
        print(f"üìÇ Loading {len(dataset_files)} datasets...")
        
        all_bills = []
        
        # Load datasets in parallel for better performance
        with ThreadPoolExecutor(max_workers=MAX_DATASETS_PARALLEL) as executor:
            future_to_dataset = {
                executor.submit(self.load_single_dataset, dataset_path): dataset_path
                for dataset_path in dataset_files
            }
            
            for future in as_completed(future_to_dataset):
                dataset_path = future_to_dataset[future]
                try:
                    bills = future.result()
                    all_bills.extend(bills)
                except Exception as e:
                    logging.error(f"Error processing dataset {dataset_path}: {e}")
        
        # Remove duplicates by bill_id
        unique_bills = {}
        for bill in all_bills:
            bill_id = bill.get('bill_id')
            if bill_id and bill_id not in unique_bills:
                unique_bills[bill_id] = bill
        
        final_bills = list(unique_bills.values())
        print(f"üìä Total unique bills loaded: {len(final_bills)}")
        
        return final_bills
    
    def filter_bills_by_keyword(self, bills, keyword):
        """Filter bills by healthcare keyword"""
        matching_bills = []
        
        for bill in bills:
            if self.bill_contains_keyword(bill, keyword):
                matching_bills.append(bill)
        
        return matching_bills
    
    def bill_contains_keyword(self, bill, keyword):
        """Check if bill contains the healthcare keyword"""
        # Search in multiple fields
        search_fields = [
            bill.get('title', ''),
            bill.get('description', ''),
            bill.get('text', ''),
            bill.get('summary', '')
        ]
        
        # Also search in sponsor information
        sponsors = bill.get('sponsors', [])
        for sponsor in sponsors:
            if isinstance(sponsor, dict):
                search_fields.append(sponsor.get('name', ''))
        
        # Combine all text and search (case-insensitive)
        search_text = ' '.join(search_fields).lower()
        return keyword.lower() in search_text
    
    def filter_bills_by_year(self, bills, target_years):
        """Filter bills by target years"""
        filtered_bills = []
        
        for bill in bills:
            session = bill.get('session', {})
            bill_year = session.get('year_start')
            
            if bill_year in target_years:
                filtered_bills.append(bill)
        
        return filtered_bills
    
    def search_bills_comprehensive(self, keyword):
        """Comprehensive search for healthcare bills"""
        # Load all datasets
        all_bills = self.load_all_datasets()
        
        if not all_bills:
            return []
        
        # Filter by keyword
        keyword_bills = self.filter_bills_by_keyword(all_bills, keyword)
        
        # Filter by target years
        final_bills = self.filter_bills_by_year(keyword_bills, TARGET_YEARS)
        
        logging.info(f"Found {len(final_bills)} bills for keyword '{keyword}'")
        return final_bills

# Test the dataset loader
if __name__ == "__main__":
    print("=" * 60)
    print("DATASET LOADER - TESTING")
    print("=" * 60)
    
    # Initialize loader
    loader = DatasetLoader()
    
    # Test loading (will show warning if no datasets)
    bills = loader.load_all_datasets()
    
    if bills:
        print(f"‚úÖ Successfully loaded {len(bills)} bills")
        
        # Test keyword search
        test_keyword = "Prior authorization"
        matching_bills = loader.filter_bills_by_keyword(bills, test_keyword)
        print(f"‚úÖ Found {len(matching_bills)} bills matching '{test_keyword}'")
    else:
        print("‚ÑπÔ∏è  No datasets available for testing")
        print("   This is normal - datasets will be added in the next step")
    
    print("\nDataset loader ready!")


#main.py


import time
import logging
from datetime import datetime

# Corporate environment SSL bypass
import urllib3
import ssl
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

from config import *
from dataset_manager import DatasetManager
from dataset_loader import DatasetLoader
from bill_processor import BillProcessor

def setup_logging():
    """Setup logging configuration"""
    import os
    os.makedirs(LOGS_DIR, exist_ok=True)
    
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE),
            logging.StreamHandler()
        ]
    )

def process_single_keyword(loader, processor, keyword):
    """Process a single healthcare keyword"""
    logging.info(f"Processing keyword: {keyword}")
    
    # Search for bills containing this keyword
    matching_bills = loader.search_bills_comprehensive(keyword)
    
    if not matching_bills:
        logging.warning(f"No bills found for keyword: {keyword}")
        return {'keyword': keyword, 'count': 0, 'error': 'No bills found'}
    
    # Process each bill
    processed_count = 0
    for bill in matching_bills:
        processed_bill = processor.process_bill_data(bill)
        if processed_bill:
            processor.add_bill(processed_bill)
            processed_count += 1
    
    return {'keyword': keyword, 'count': processed_count, 'error': None}

def main():
    """Main healthcare bills extraction workflow"""
    start_time = time.time()
    
    print("=" * 70)
    print("HEALTHCARE BILLS DATASET EXTRACTION SYSTEM")
    print("=" * 70)
    
    setup_logging()
    logging.info("Starting healthcare bills extraction from datasets")
    
    # Initialize components
    dataset_manager = DatasetManager()
    dataset_loader = DatasetLoader()
    bill_processor = BillProcessor()
    
    # Check available datasets
    available_datasets = dataset_manager.get_available_datasets()
    if not available_datasets:
        print("‚ùå No datasets found!")
        print("   Please download datasets from https://legiscan.com/datasets")
        print("   Save them in the 'datasets/' folder with format: YYYY-MM-DD-legiscan.json")
        return
    
    print(f"üìÇ Found {len(available_datasets)} datasets")
    print(f"üîç Processing {len(KEYWORDS)} healthcare keywords...")
    print(f"üìÖ Target years: {TARGET_YEARS}")
    print("-" * 70)
    
    # Process each healthcare keyword
    search_results_summary = {}
    
    for i, keyword in enumerate(KEYWORDS, 1):
        print(f"[{i}/{len(KEYWORDS)}] Processing: '{keyword}'")
        
        try:
            result = process_single_keyword(dataset_loader, bill_processor, keyword)
            search_results_summary[keyword] = result['count']
            
            if result['error']:
                print(f"   ‚ö†Ô∏è  {result['error']}")
            else:
                print(f"   ‚úÖ Found {result['count']} bills")
                
        except Exception as e:
            print(f"   ‚ùå Error processing '{keyword}': {e}")
            logging.error(f"Error processing keyword {keyword}: {e}")
            search_results_summary[keyword] = 0
    
    # Calculate processing time
    processing_time = time.time() - start_time
    
    print("=" * 70)
    print("EXTRACTION COMPLETE")
    print("=" * 70)
    
    # Show comprehensive summary
    total_bills = len(bill_processor.processed_bills)
    print(f"üìä Total bills found: {total_bills}")
    print(f"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds ({processing_time/60:.1f} minutes)")
    print(f"üìà Summary: {bill_processor.get_summary()}")
    
    # Show processing statistics
    processing_stats = bill_processor.get_processing_stats()
    
    print(f"\nüìã Processing Statistics:")
    print(f"   Bills processed: {processing_stats['successful']}")
    print(f"   Processing failures: {processing_stats['failed']}")
    print(f"   Duplicates removed: {processing_stats['duplicates_removed']}")
    
    # Show keyword breakdown
    print(f"\nüîç Keyword Breakdown:")
    for keyword, count in search_results_summary.items():
        print(f"   ‚Ä¢ {keyword}: {count} bills")
    
    # Save results to Excel
    if total_bills > 0:
        print(f"\nüíæ Saving results to Excel...")
        bill_processor.save_to_excel()
        
        # Calculate processing rate
        if processing_time > 0:
            rate = total_bills / processing_time
            print(f"üìä Processing rate: {rate:.2f} bills/second")
        
        print(f"\n‚úÖ Extraction complete! Results saved to: {OUTPUT_FILE}")
    else:
        print(f"\n‚ö†Ô∏è  No bills found matching your healthcare keywords")
        print("   Check that your datasets contain 2025-2026 data")
    
    print(f"üìù Logs saved to: {LOG_FILE}")
    logging.info(f"Extraction complete. Total bills: {total_bills}, Time: {processing_time:.2f}s")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Extraction interrupted by user.")
        logging.info("Extraction interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Unexpected error: {e}")
        logging.error(f"Unexpected error: {e}")
        raise




###keywords

def check_keyword_match(self, bill_data, target_keyword):
    """Enhanced keyword matching with flexibility"""
    if not bill_data:
        return False, target_keyword
    
    # Get the bill data
    bill = bill_data.get('bill', {}) if 'bill' in bill_data else bill_data
    
    # Fields to search in
    search_fields = [
        bill.get('title', ''),
        bill.get('description', ''),
        bill.get('summary', ''),
        bill.get('text', ''),
    ]
    
    # Search in history/actions
    history = bill.get('history', [])
    if isinstance(history, list):
        for action in history:
            if isinstance(action, dict):
                search_fields.append(action.get('action', ''))
    
    # Search in sponsors
    sponsors = bill.get('sponsors', [])
    if isinstance(sponsors, list):
        for sponsor in sponsors:
            if isinstance(sponsor, dict):
                search_fields.append(sponsor.get('name', ''))
    
    # Combine all text and search (case-insensitive)
    combined_text = ' '.join(search_fields).lower()
    
    # Enhanced keyword matching
    keyword_variations = [
        target_keyword.lower(),
        target_keyword.lower().replace(' review', ''),  # "utilization" matches "utilization review"
        target_keyword.lower().replace(' ', ''),        # Handle spacing issues
    ]
    
    # Check for any variation
    for variation in keyword_variations:
        if variation in combined_text:
            return True, target_keyword
    
    # If strict matching fails, trust API results for now
    logging.info(f"Keyword '{target_keyword}' not found in bill {bill.get('bill_number', 'unknown')} - trusting API")
    return True, target_keyword  # Trust API search results
